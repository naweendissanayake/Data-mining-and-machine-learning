# Data-mining-and-machine-learning
This project was a kaggle competion project that we succusfully achieve nice result during our traing and we were give to check which algorithm works perfectly for this dataset and why others are not woking: I will attach my report as well to see you. 
The goal of the assignment is to gain experience with a number of explorative data analysis techniques, learning, pre-processing, learning and tuning algorithms we have learned about and optionally beyond that. A dataset with known class values ("X_train.csv", "y_train.csv") is available on Moodle. Download the dataset, explore it and generate models using Python (or Weka).

We organised an online competition at kaggle.com for the best model. You have to make predictions for the second dataset ("X_test.csv") and upload your predictions to the Kaggle competition site. We also provided you an example for the solution file needed by Kaggle (y_test_pred_dectree_md2.csv).  Please register on the Kaggle site and join the competition using the invitation link. Please use your real name or let us know on the frontpage of your report, what your Kaggle user name is.

You must hand in a report  with an overview of the data, the algorithms tested, your considerations for choosing and tuning the algorithms, the achieved prediction performance values, your conclusions and experiences. 

Due Date
1st version, minimum requirements for the signature:
deadline: 10th December (Saturday), 11:59pm:
5 algorithms, incl. at least one data preparation or hyperparameter tuning method
2 Kaggle uploads with score > 0.50
Report (pdf) >= 2 pages
Final version, requirements:
deadline:  three days before your exam date, 11:59 pm
all algorithms (incl. clustering, probably no association rule mining and regression algorithms; nearly all algorithms)
Kaggle score > 0.76
Report >=10 pages


You have to upload your files in Moodle (NEPTUNCODE and SURNAME have to be replaced with your data, all in capitals):

report as a single pdf file named as “SURNAME_NEPTUNCODE.pdf”

Please write all your notes in the pdf report, additional information in the mail will be ignored! 

Please put also your Kaggle user name on the title page (unless you use your real name on Kaggle)

If you use Python, please also attach one or more Jupyter notebook files (SURNAME_NEPTUNCODE.ipynb ) with your code.

The notebook files should contain the output and be runnable by us.

If you have multiple notebook files, please

extend the filename, in a meaningful way (e.g. SURNAME_NEPTUNCODE_dataexploration.ypnb),

The evaluation of the assignment will be based on:
Report including among others: exploratory data analysis, data preprocessing, algorithms, tuning, prediction performance, overview of all relevant experiments and performance measures, conclusions

Ideas behind / number of algorithms/variations, parameter tuning tested/submitted

Elegance of model (“whitebox”/simple models are preferred -- especially if prediction performance is not degraded)

(Prediction) performance of the developed algorithm

Rules
Be aware of the competition rules described at the competition page on Kaggle.
Only the dataset provided at the dedicated Kaggle competition shall be used. If you use external datasets, your assignment is invalid and there is no second chance.

We must be able to reproduce your solution on the basis of settings and parameter values you used with Weka or on the basis of your Jupyter notebook(s).

If the algorithm uses random values, the seeds must be fixed and documented.

If you use program code, the directory for the input files must be defined at the beginning

In the documentation, a general description of the algorithms discussed in the lectures is not needed. Your considerations for choosing or tuning an algorithm should be documented. If you use algorithms not discussed, a short description is needed (+ reference literature).

Results generated by automated ML tools (e.g., Google AutoML, Auto-Sklearn) are not allowed at the competition. After the competitions’ deadline you can upload such results as late submissions and include those and your experiences with automated tools in your report, but only as an extension to your normal work. 

Please do not hesitate to contact me if you have any questions, difficulties! Also, if there are problems with the contest evaluation, please drop me a mail.

Description of Dataset “APS Failure at Scania Trucks”
General description
The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that are utilized in various functions in a truck, such as braking and gear changes. The dataset’s positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts. The goal is to predict whether the APS system has a failure and should be repaired, too.  A false positive prediction means that the mechanics checks the APS system unnecessarily, a false negative prediction that it is faulty, but not checked. Obviously, the latter has larger negative consequences.

Attribute Information

The attribute names of the data have been anonymized for proprietary reasons. It consists of both single numerical counters and histograms consisting of bins with different conditions. Typically the histograms have open-ended conditions at each end. For example, if we measuring the ambient temperature 'T' then the histogram could be defined with 4 bins where:

bin 1 collect values for temperature T < -20
bin 2 collect values for temperature T >= -20 and T < 0
bin 3 collect values for temperature T >= 0 and T < 20
bin 4 collect values for temperature T > 20

| b1 | b2 | b3 | b4 |
-----------------------------
-20 0 20

The attributes are as follows:  anonymized operational data. The operational data have an identifier and a bin id, like 'Identifier_Bin'.  In total there are 171 attributes, of which 7 are histogram variables.

Evaluation measure
The evaluation of the result uses the F3 score. In general, both precision and recall are important, which suggests to use an F score. As recall (catching many positives) is more important the precision (not to have negatives in the result set), we used the F3 score.  S. https://en.wikipedia.org/wiki/F-score

Remark: A cost-based evaluation with a cost-matrix would be a sensible alternative. However, for various technical reasons, we decided for the F3 score for this assignment and the accompanying Kaggle competition.
